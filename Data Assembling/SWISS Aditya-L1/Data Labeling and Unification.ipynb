{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e46b7a-5951-4e38-9931-9383c57db368",
   "metadata": {},
   "source": [
    "# Data Labeling and Unification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7a3aa-ac1d-4933-99c7-4c5fee5f7ffb",
   "metadata": {},
   "source": [
    "Here we combine the data from cactus and Adtiya L1-SWISS to create one HDF5 file which contains all the features from level 2, from all three files BLK,TH1 and TH2. \n",
    "\n",
    "We also label them as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3acc4a-3cdd-43ba-82aa-6c1e2c157777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Store ALL SWIS Level-2 CDF variables (including multi-D)\n",
    "in a single HDF5 file, grouped by date, with event labels\n",
    "and a global summary. Handles scalar vs array datasets correctly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from cdflib import CDF\n",
    "try:\n",
    "    from cdflib.epochs import CDFepoch\n",
    "except ImportError:\n",
    "    from cdflib.cdfepoch import CDFepoch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tables  # for clearing stale HDF5 handles\n",
    "\n",
    "# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_DIR     = \"/Volumes/Samsung_PSSD_T7_Shield/Aditya L1 Swiss/pradan1.issdc.gov.in/al1/protected/downloadData/aspex/swis/level2\"\n",
    "CACTUS_CSV   = \"/Volumes/Samsung_PSSD_T7_Shield/Aditya L1 Swiss/Cactus/cactus_all_cmes.csv\"\n",
    "OUTPUT_FILE  = \"/Volumes/Samsung_PSSD_T7_Shield/Aditya L1 Swiss/sw_level2_full.h5\"\n",
    "ONSET_COL    = \"onset_time_utc\"\n",
    "WIDTH_COL    = \"angular_width_deg\"\n",
    "MATCH_WINDOW = timedelta(minutes=30)\n",
    "\n",
    "# â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_cactus(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['event_time'] = pd.to_datetime(\n",
    "        df[ONSET_COL], format=\"%Y/%m/%d %H:%M\", utc=True\n",
    "    )\n",
    "    def cat(w):\n",
    "        if pd.isna(w) or w == 0:      return 0\n",
    "        if w < 20:                     return 1\n",
    "        if w < 60:                     return 2\n",
    "        if w < 120:                    return 3\n",
    "        return 4\n",
    "    df['label_code'] = df[WIDTH_COL].apply(cat)\n",
    "    print(f\"âœ”ï¸  Loaded {len(df)} CACTUS events\")\n",
    "    return df[['event_time','label_code']]\n",
    "\n",
    "def read_cdf_vars(path):\n",
    "    \"\"\"Return (vars_dict, time_var_name) for a CDF file.\"\"\"\n",
    "    cdf = CDF(path)\n",
    "    info = cdf.cdf_info()\n",
    "    out = {v: cdf.varget(v) for v in info.zVariables}\n",
    "    # pick the first variable containing \"epoch\"\n",
    "    time_vars = [v for v in out if 'epoch' in v.lower()]\n",
    "    if not time_vars:\n",
    "        raise KeyError(f\"No 'epoch' variable found in {path}\")\n",
    "    return out, time_vars[0]\n",
    "\n",
    "# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    cactus = load_cactus(CACTUS_CSV)\n",
    "    stats = dict(cdf_files=0, triplets=0, timestamps=0, labeled=0)\n",
    "\n",
    "    # gather all UNP_9999 dirs\n",
    "    unp_dirs = []\n",
    "    for yr in (\"2024\",\"2025\"):\n",
    "        for m in range(1,13):\n",
    "            d = os.path.join(BASE_DIR, yr, f\"{m:02d}\", \"UNP_9999\")\n",
    "            if os.path.isdir(d):\n",
    "                unp_dirs.append(d)\n",
    "    print(f\"ðŸ“‚ Found {len(unp_dirs)} UNP_9999 folders\\n\")\n",
    "\n",
    "    # clear stale HDF5 handles and remove old file\n",
    "    tables.file._open_files.close_all()\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "\n",
    "    with h5py.File(OUTPUT_FILE, 'w') as hf:\n",
    "        stat_grp = hf.create_group('stats')\n",
    "\n",
    "        for unp in tqdm(unp_dirs, desc=\"Folders\"):\n",
    "            cdfs = glob.glob(os.path.join(unp, \"*.cdf\"))\n",
    "            stats['cdf_files'] += len(cdfs)\n",
    "\n",
    "            # extract all YYYYMMDD dates from filenames\n",
    "            dates = sorted({\n",
    "                re.search(r\"_(\\d{8})_\", os.path.basename(f)).group(1)\n",
    "                for f in cdfs if re.search(r\"_(\\d{8})_\", os.path.basename(f))\n",
    "            })\n",
    "\n",
    "            for dt in dates:\n",
    "                # pick BLK/TH1/TH2 files, prefer V02\n",
    "                trip = {}\n",
    "                for inst in (\"BLK\",\"TH1\",\"TH2\"):\n",
    "                    pat = f\"_L2_{inst}_{dt}_\"\n",
    "                    v02 = [f for f in cdfs if pat in f and \"_V02.cdf\" in f]\n",
    "                    v01 = [f for f in cdfs if pat in f and \"_V01.cdf\" in f]\n",
    "                    trip[inst] = v02[0] if v02 else (v01[0] if v01 else None)\n",
    "                if None in trip.values():\n",
    "                    continue\n",
    "                stats['triplets'] += 1\n",
    "\n",
    "                # read variables and detect time axis\n",
    "                blk_vars, blk_tv = read_cdf_vars(trip['BLK'])\n",
    "                th1_vars, th1_tv = read_cdf_vars(trip['TH1'])\n",
    "                th2_vars, th2_tv = read_cdf_vars(trip['TH2'])\n",
    "                assert blk_tv == th1_tv == th2_tv\n",
    "                tv = blk_tv\n",
    "\n",
    "                # convert epochs â†’ UTC datetimes\n",
    "                times = CDFepoch.to_datetime(blk_vars[tv])\n",
    "                times = pd.to_datetime(times).tz_localize('UTC')\n",
    "                stats['timestamps'] += len(times)\n",
    "\n",
    "                # build label array\n",
    "                labels = np.zeros(len(times), dtype='i4')\n",
    "                for evt, code in zip(cactus['event_time'], cactus['label_code']):\n",
    "                    mask = ((times >= evt - MATCH_WINDOW) &\n",
    "                            (times <= evt + MATCH_WINDOW))\n",
    "                    labels[mask] = code\n",
    "                stats['labeled'] += int((labels != 0).sum())\n",
    "\n",
    "                # write date group\n",
    "                grp = hf.require_group(dt)\n",
    "                # store times in ms since epoch\n",
    "                ms = (times.view('int64') // 1_000_000).astype('i8')\n",
    "                grp.create_dataset('utc_time_ms', data=ms, dtype='i8')\n",
    "                grp.create_dataset('label_code', data=labels, dtype='i4')\n",
    "\n",
    "                # write each instrument's variables (multi-D OK)\n",
    "                for inst, varset in (('BLK',blk_vars),\n",
    "                                     ('TH1',th1_vars),\n",
    "                                     ('TH2',th2_vars)):\n",
    "                    ig = grp.require_group(inst)\n",
    "                    for name, arr in varset.items():\n",
    "                        if np.ndim(arr) == 0:\n",
    "                            ig.create_dataset(name, data=arr)\n",
    "                        else:\n",
    "                            ig.create_dataset(name,\n",
    "                                              data=arr,\n",
    "                                              compression='gzip',\n",
    "                                              compression_opts=4)\n",
    "\n",
    "        # attach global stats as attributes\n",
    "        for k, v in stats.items():\n",
    "            stat_grp.attrs[k] = v\n",
    "\n",
    "    # final summary print\n",
    "    print(\"\\nðŸ“Š **Summary**\")\n",
    "    print(f\"  â€¢ .cdf files scanned:    {stats['cdf_files']}\")\n",
    "    print(f\"  â€¢ Triplets processed:    {stats['triplets']}\")\n",
    "    print(f\"  â€¢ Timestamps stored:     {stats['timestamps']}\")\n",
    "    print(f\"  â€¢ Timestamps labeled:    {stats['labeled']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c92bab2-1270-4b6f-bd9c-9487a6f2d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /20240801 (dataset paths, shapes, dtypes):\n",
      "\n",
      "BLK/alpha_bulk_speed   shape=(17275,)   dtype=float64\n",
      "BLK/alpha_density   shape=(17275,)   dtype=float64\n",
      "BLK/alpha_thermal   shape=(17275,)   dtype=float64\n",
      "BLK/bulk_a_uncer   shape=(17275,)   dtype=float64\n",
      "BLK/bulk_p_uncer   shape=(17275,)   dtype=float64\n",
      "BLK/epoch_for_cdf_mod   shape=(17275,)   dtype=float64\n",
      "BLK/numden_a_uncer   shape=(17275,)   dtype=float64\n",
      "BLK/numden_p_uncer   shape=(17275,)   dtype=float64\n",
      "BLK/proton_bulk_speed   shape=(17275,)   dtype=float64\n",
      "BLK/proton_density   shape=(17275,)   dtype=float64\n",
      "BLK/proton_thermal   shape=(17275,)   dtype=float64\n",
      "BLK/proton_xvelocity   shape=(17275,)   dtype=float64\n",
      "BLK/proton_yvelocity   shape=(17275,)   dtype=float64\n",
      "BLK/proton_zvelocity   shape=(17275,)   dtype=float64\n",
      "BLK/spacecraft_xpos   shape=(17275,)   dtype=float64\n",
      "BLK/spacecraft_ypos   shape=(17275,)   dtype=float64\n",
      "BLK/spacecraft_zpos   shape=(17275,)   dtype=float64\n",
      "BLK/thermal_a_uncer   shape=(17275,)   dtype=float64\n",
      "BLK/thermal_p_uncer   shape=(17275,)   dtype=float64\n",
      "TH1/energy_center_mod   shape=(17275, 50)   dtype=float64\n",
      "TH1/energy_uncer   shape=()   dtype=float64\n",
      "TH1/epoch_for_cdf_mod   shape=(17275,)   dtype=float64\n",
      "TH1/flux_uncer   shape=()   dtype=float64\n",
      "TH1/integrated_flux_mod   shape=(17275, 50)   dtype=float64\n",
      "TH1/integrated_flux_s10_mod   shape=(17275, 50)   dtype=float64\n",
      "TH1/integrated_flux_s11_mod   shape=(17275, 50)   dtype=float64\n",
      "TH1/integrated_flux_s9_mod   shape=(17275, 50)   dtype=float64\n",
      "TH1/spacecraft_xpos   shape=(17275,)   dtype=float64\n",
      "TH1/spacecraft_ypos   shape=(17275,)   dtype=float64\n",
      "TH1/spacecraft_zpos   shape=(17275,)   dtype=float64\n",
      "TH1/sun_angle_tha1   shape=(17275, 16, 3)   dtype=float64\n",
      "TH2/energy_center_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/energy_uncer   shape=()   dtype=float64\n",
      "TH2/epoch_for_cdf_mod   shape=(17275,)   dtype=float64\n",
      "TH2/flux_uncer   shape=()   dtype=float64\n",
      "TH2/integrated_flux_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/integrated_flux_s15_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/integrated_flux_s16_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/integrated_flux_s17_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/integrated_flux_s18_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/integrated_flux_s19_mod   shape=(17275, 50)   dtype=float64\n",
      "TH2/spacecraft_xpos   shape=(17275,)   dtype=float64\n",
      "TH2/spacecraft_ypos   shape=(17275,)   dtype=float64\n",
      "TH2/spacecraft_zpos   shape=(17275,)   dtype=float64\n",
      "TH2/sun_angle_tha2   shape=(17275, 32, 3)   dtype=float64\n",
      "label_code   shape=(17275,)   dtype=int32\n",
      "utc_time_ms   shape=(17275,)   dtype=int64\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import h5py\n",
    "\n",
    "def print_datasets(grp, parent_path=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively print every dataset under grp, showing:\n",
    "      <full_path>   shape=<shape>   dtype=<dtype>\n",
    "    \"\"\"\n",
    "    for name, item in grp.items():\n",
    "        path = f\"{parent_path}/{name}\" if parent_path else name\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            print(f\"{path}   shape={item.shape}   dtype={item.dtype}\")\n",
    "        else:  # Group\n",
    "            print_datasets(item, path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fp = \"sw_level2_full.h5\"   # adjust if needed\n",
    "    with h5py.File(fp, \"r\") as hf:\n",
    "        date = \"20240801\"\n",
    "        if date not in hf:\n",
    "            raise KeyError(f\"Group '{date}' not found\")\n",
    "        print(f\"Contents of /{date} (dataset paths, shapes, dtypes):\\n\")\n",
    "        print_datasets(hf[date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaaabd0-2e08-443d-96ad-c5ca694b3c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9613021-5d1e-4f7f-9917-6bfc8d3aee42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
